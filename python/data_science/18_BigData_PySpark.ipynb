{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets that are smaller than 8 Gb can be processed **locally**, but when the data exceeds that limit, we need to start using **distributed systems**. On distributed systems, one machine (main node) controls how to distribute the data aross several machines (worker nodes); thus, we **scale** the processing power and become **fault-tolerant**: if a node/machine fails, the model/system still works distributed on the rest.\n",
    "\n",
    "### Hadoop: HDFS & MapReduce\n",
    "\n",
    "Apache **Hadoop** is often used in such distrubuted systems. Hadoop is a collection of open-source software utilities for computer networks that process massive amounts of data.\n",
    "- Hadoop uses **HDFS: Hadoop Distributed File system**; these allow distributuíng large datasets across several machines\n",
    "- As HDFS is used to distrubute data, **MapReduce** is used to distribute computation on HDFS data\n",
    "\n",
    "More details on HDFS & MapReduce:\n",
    "- HDFS distributes the data in blocks (of 128 MB by default) for parallelization, and replicates each block 3x; blocks are distributed for fault-tolerance\n",
    "- MapReduce distributes a computation task to a distributed set of files\n",
    "    - It has one **job tracker** (in the main node) and **task trackers** on the distributed nodes\n",
    "    - Job trackers send code to run to the task trackers\n",
    "    - Task trackers allocate CPU + RAM and monitor the tasks on the worker nodes\n",
    "    \n",
    "An alternative to MapReduce is **Spark**, a technology gaining popularity for Big Data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark\n",
    "\n",
    "Apache Spark is an open source general pupose cluster computing framework, like an alternative for MapReduce, but which can be still used with Hadoop. It supports data stored in many formats:\n",
    "- Cassandra\n",
    "- AWS S3\n",
    "- HDFS\n",
    "- and more\n",
    "\n",
    "Spark is 100x faster than MapReduce because it keeps the data in the RAM, whereas MapReduce writes the data to disk after eahc transformation.\n",
    "\n",
    "Spark works with **Resilient Distributed Datasets (RDDs)**.\n",
    "The conceptual functioning is as explained for Hadoop: we have a cluster manager node which controls worker nodes; all nodes are additionally interfaced by a driver program with a SparkContext.\n",
    "However, the users interact usually with RDD obejcts.\n",
    "RDDs have 4 main features:\n",
    "- Distributed Collection of Data\n",
    "- Fault-tolerant\n",
    "- Parallel operations\n",
    "- Ability to use many data sources\n",
    "\n",
    "RDDs perform basically two types of operations:\n",
    "- Transformations (=recipies to follow, operations that produce RDDs); common examples:\n",
    "    - RDD.filter(): apply a function to each element and return elements that evaluate to true\n",
    "    - RDD.map(): transform elements with a function and return same number of elements; like pd.apply()\n",
    "        - Example: grab first letter from a list of names\n",
    "    - RDD.flatMap(): transform each element (e.g., a tuple/array) into N elements; number of total elements is changed\n",
    "        - Example: transform a corpus of text into a list of words\n",
    "- Actions (= operations that produce local objects); common examples:\n",
    "    - Collect: return all the elements of the RDD as an array\n",
    "        - An action can be executed on a transformation of a RDD; RDDs are lazily evaluated, i.e., we basically generate generators with lambda functions in form of transformations, and when we perform actions (e.g., collect), these are evaluated\n",
    "    - Count: Return number of elements in the RDD\n",
    "    - First: Return first element in the RDD\n",
    "    - Take: Return an array with the first n elements in the RDD\n",
    "\n",
    "We have also Pair RDDs, which contain key-value pairs; these have additionally these operations, which are similar to pd.groupby():\n",
    "- Reduce: aggregate RDD elements and return a single element\n",
    "- ReduceByKey\n",
    "\n",
    "The Spark ecosystem is expanding very fast; now, there are: Spark SQL, Spark DataFrames, MLlib, Spark Streaming, ...\n",
    "\n",
    "**Important: We can use Spark on our local machine to learn (pnly on Ubuntu), but it only makes sense to use it on a cloud service, like AWS. If we plan to work locally, it makes more sense to work with pandas or SQL.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Web Services (AWS)\n",
    "\n",
    "Create a new free account on AWS:\n",
    "https://aws.amazon.com/free/\n",
    "\n",
    "**IMPORTANT: It's free as long as we don't exceed the usage limits**\n",
    "\n",
    "We create an Amazon EC2 instance on AWS: that is a virtual machin to which we connect from our local computer via SSH.\n",
    "\n",
    "#### Configuration Steps:\n",
    "- Log in\n",
    "- Select in AWS Services: Services: Compute: EC2\n",
    "- Launch/Create Instance\n",
    "- Choose Amazon Machine Image = OS we want\n",
    "- Choose Free Tier, Ubuntu\n",
    "- Choose an instance type (free elegible): t2.micro; we see instance types vary in RAM: larger ones have 100+ Gb!\n",
    "- Configure instance details\n",
    "    - Make sure we have one unique instance\n",
    "    - We can leave rest as default\n",
    "- Add storage: default 8 Gb, leave it like that\n",
    "- Add a Tag key-value: myspark-mymachine\n",
    "- Create new security group\n",
    "    - Type: default is SSH, we change it to All traffic, which opens all ports; **IMPORTANT: This is for educational purposes only, do not open all ports on production!**\n",
    "    - Source: anywhere\n",
    "- Review instance launch\n",
    "- Launch\n",
    "- Key pair (private key):\n",
    "    - Create a new key pair: newspark\n",
    "    - Download key pair: save it securely and don't loose it, otherwise we need to go through all the configuration steps again; the file is `newspark.pem` -> `~/keys/aws/newspark.pem`\n",
    "- Launch instance(s); then, click on instance link and we go to the Amazon EC2 Dashboard\n",
    "\n",
    "#### Amazon EC2 Console:\n",
    "- We see the instances here\n",
    "- Most important button: Actions > Instance State > Terminate; we can start instances again!\n",
    "\n",
    "#### Connection Steps (from local computer):\n",
    "- For windows: Putty -> google('ssh windows ec2')\n",
    "    - Basically 2 Putty execuatbles are downloaded: Putty and PuttyGen; the latter is used to convert the downloaded private key into Putty's format\n",
    "    - Putty is launched and we create a new session\n",
    "        - SSH\n",
    "        - username@dns (from Amazon EC2 dashboard)\n",
    "        - Connection, SSH, Authorization: we select our converted ley\n",
    "- For Linux/Mac: open AWS EC2 console and get the DNS of the virtual machine, and\n",
    "```bash\n",
    "    # go to folder with key\n",
    "    cd ~/keys/aws\n",
    "    # make key readable only, to avoid overwriting it (it's compulsory, otherwise doesn't work)\n",
    "    chmod 400 newspark.pem\n",
    "    # connect to virtual EC2 instance\n",
    "    ssh -i ~/keys/aws/newspark.pem ubuntun@DNS\n",
    "```\n",
    "- Once connected per SSH, we need to set up our virtual machine: install stuff, etc.\n",
    "\n",
    "#### PySpark Setup\n",
    "\n",
    "Connect to the virtual machine.\n",
    "\n",
    "https://medium.com/@josemarcialportilla/getting-spark-python-and-jupyter-notebook-running-on-amazon-ec2-dec599e1c297\n",
    "\n",
    "**IMPORTANT NOTE**: I have not managed to make it work following these steps.\n",
    "\n",
    "```bash\n",
    "\n",
    "# Connect to virtual EC2 instance\n",
    "ssh -i ~/keys/aws/newspark.pem ubuntu@DNS\n",
    "    \n",
    "# Install Anaconda: Look in the link which is the latest version\n",
    "# wget http://repo.continuum.io/archive/Anaconda3-4.1.1-Linux-x86_64.sh\n",
    "# bash Anaconda3–4.1.1-Linux-x86_64.sh\n",
    "wget http://repo.continuum.io/archive/Anaconda3-2020.11-Linux-x86_64.sh\n",
    "bash Anaconda3-2020.11-Linux-x86_64.sh\n",
    "\n",
    "# Set paths, initialize\n",
    "eval \"$(/home/ubuntu/anaconda3/bin/conda shell.bash hook)\"\n",
    "conda init\n",
    "\n",
    "# Check we're using the Anaconda python\n",
    "which python\n",
    "which python3\n",
    "\n",
    "# If not the Anaconda version:\n",
    "source ~/.bashrc\n",
    "\n",
    "# Configurate jupyter notebook\n",
    "jupyter notebook --generate-config\n",
    "\n",
    "# Create certifications\n",
    "mkdir ~/certs\n",
    "cd ~/certs\n",
    "sudo openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout mycert.pem -out mycert.pem\n",
    "sudo su\n",
    "chown ubuntu:ubuntu mycert.pem\n",
    "\n",
    "# Edit Jupyter config file\n",
    "cd ~/.jupyter/\n",
    "vim jupyter_notebook_config.py\n",
    "\n",
    "    c = get_config()\n",
    "\n",
    "    # Notebook config this is where you saved your pem cert\n",
    "    c.NotebookApp.certfile = u'/home/ubuntu/certs/mycert.pem' \n",
    "\n",
    "    # Run on all IP addresses of your instance\n",
    "    c.NotebookApp.ip = '*'\n",
    "\n",
    "    # Don't open browser by default\n",
    "    c.NotebookApp.open_browser = False  \n",
    "\n",
    "    # Fix port to 8888\n",
    "    c.NotebookApp.port = 8888\n",
    "\n",
    "# Start Jupyter Notebook\n",
    "jupyter notebook\n",
    "\n",
    "# Remove Anaconda installation file\n",
    "cd ~\n",
    "# rm -fr Anaconda3–4.1.1-Linux-x86_64.sh\n",
    "rm -fr Anaconda3-2020.11-Linux-x86_64.sh\n",
    "\n",
    "## Test that Jupyter server is accessible from local computer\n",
    "\n",
    "# Open on local machine (Firefox) the address/DNS of the virtual machine\n",
    "# With port 8888 to access the Jupyter notebook instance\n",
    "# If warning, Accept Risk and Continue\n",
    "# Taken token from ubuntu Terminal\n",
    "https://ec2-xx-xx-xxx-xxx.us-west-2.compute.amazonaws.com:8888\n",
    "\n",
    "## Back in the Ubuntu/AWS Terminal\n",
    "\n",
    "# Install Java\n",
    "sudo apt-get update\n",
    "sudo apt-get install default-jre\n",
    "\n",
    "# Check it worked\n",
    "java -version\n",
    "\n",
    "# Install Scala\n",
    "sudo apt-get install scala\n",
    "\n",
    "# Check it worked\n",
    "scala -version\n",
    "\n",
    "# Optional: For specific versions of Scala\n",
    "# wget http://www.scala-lang.org/files/archive/scala-2.11.8.deb\n",
    "# sudo dpkg -i scala-2.11.8.deb\n",
    "\n",
    "# Install pip\n",
    "export PATH=$PATH:$HOME/anaconda3/bin\n",
    "# echo export PATH=\\$PATH:\\$HOME/anaconda3/bin >> ~/.bashrc\n",
    "conda install pip\n",
    "which pip\n",
    "\n",
    "# Install py4j (python-java connection)\n",
    "pip install py4j\n",
    "\n",
    "# Install Spark and Hadoop (browse the version we'd like first)\n",
    "cd ~\n",
    "# wget http://archive.apache.org/dist/spark/spark-2.0.0/spark-2.0.0-bin-hadoop2.7.tgz\n",
    "# sudo tar -zxvf spark-2.0.0-bin-hadoop2.7.tgz\n",
    "wget http://archive.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
    "sudo tar -zxvf spark-2.4.7-bin-hadoop2.7.tgz\n",
    "\n",
    "# Add these lines to ~/.bashrc\n",
    "# export SPARK_HOME='/home/ubuntu/spark-2.0.0-bin-hadoop2.7'\n",
    "export SPARK_HOME='/home/ubuntu/spark-2.4.7-bin-hadoop2.7'\n",
    "export PATH=$SPARK_HOME:$PATH\n",
    "export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH\n",
    "source ~/.bashrc \n",
    "\n",
    "# Remove Spark installation files\n",
    "# rm -fr spark-2.0.0-bin-hadoop2.7.tgz\n",
    "rm -fr spark-2.4.7-bin-hadoop2.7.tgz\n",
    "\n",
    "# I had a version conflict between Spark and Python and I had to downgrade python\n",
    "# Look for available versions\n",
    "conda search python\n",
    "# Downgrade\n",
    "conda install python=3.5.2\n",
    "\n",
    "# Test Jupyter: close all open instances and start a new one\n",
    "jupyter notebook\n",
    "\n",
    "# Open in Firefox\n",
    "https://ec2-xx-xx-xxx-xxx.us-west-2.compute.amazonaws.com:8888\n",
    "\n",
    "# Start new notebook and type\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOCAL PySpark Setup\n",
    "\n",
    "Obviously, installing pyspark locally only makes sense for leanring purposes.\n",
    "Tutorial for installing PySpark locally:\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/installation-of-pyspark\n",
    "\n",
    "1. Download & install Java: \n",
    "https://www.oracle.com/java/technologies/javase-downloads.html\n",
    "\n",
    "2. Download Spark + Hadoop\n",
    "\n",
    "```bash\n",
    "cd ~/nexo/packages/foreign/spark\n",
    "tar -xzf spark-2.4.7-bin-hadoop2.7.tgz\n",
    "```\n",
    "\n",
    "3. Install & Setup\n",
    "\n",
    "```bash\n",
    "# 3.1 py4j\n",
    "conda install py4j\n",
    "pip/3 install py4j\n",
    "\n",
    "# 3.2 pyspark\n",
    "pip/3 install pyspark\n",
    "\n",
    "# 3.3 edit bash_profile\n",
    "vim ~/.bash_profile\n",
    "# note: replace ~ by complete home path\n",
    "export SPARK_HOME='~/nexo/packages/foreign/spark/spark-2.4.7-bin-hadoop2.7'\n",
    "export PATH=$SPARK_HOME/bin:$PATH\n",
    "export PYSPARK_PYTHON=python3\n",
    "source ~/.bash_profile\n",
    "```\n",
    "**IMPORTANT NOTE**: I don't know if steps 2 and 3.3 are necessary at all; after carrying them out, `pyspark` wasn't found by my python kernel and I ended up adding step 3.2, after which pyspark could be found..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: The code below is correct, but I have not managed to make it run on my local machine for testing because the installation is not correct afetr following the steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a Spark context\n",
    "# Connection to the Spark Cluster\n",
    "# We can use it to create RDDs and broadcast variables on that cluster\n",
    "# We can instantiate only one context the way we're running things here\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a text file to work with it\n",
    "# We use th emagic command writefile - no spaces before and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing example.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile example.txt\n",
    "first line\n",
    "second line\n",
    "third line\n",
    "fourth line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create our RDD with the context\n",
    "# We create RDDs by loading files: TXT, CSV, HDFS, etc\n",
    "textFile = sc.textFile('example.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
