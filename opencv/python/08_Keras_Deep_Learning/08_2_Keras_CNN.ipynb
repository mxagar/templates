{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Deep Learning with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Convolutional Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hubel & Wiesel won in 1981 the Nobel prize for showing how the visual cortex of a mammal processes images:\n",
    "- Neurons focus on specific local regions of the image, not the complete image\n",
    "- Neurons activate to particular features in the image\n",
    "LeCun published in 1998 LeNet-5, introducing convolutional neural networks.\n",
    "\n",
    "We use **tensors** (n-dimensional arrays of arrays): scalar -> vector/array -> matrix -> tensor; with images, a typical example could be: `[Images, Height, Width, Channel]`.\n",
    "\n",
    "**Dense vs Convolutional layers**:\n",
    "- In a densly or fully connected layer, all neurons are connected\n",
    "- In a CNN, we have kernels or filters that sweep the image with convolution operations. Pixel regions under a filter location are summarized to one value, which is the input of a neuron in the next layer; hence, only the local/neighboring pixels (= neurons) under a convolutional filter are connected, not the pixels of the complete image!\n",
    "- Additionally, in each convolutional layer, we can have `N` filters; thus, the output is a **feature map** of `N` channels.\n",
    "\n",
    "We call **feedforward** pass to the action of inputing an image to a CNN to obtain the predicted class.\n",
    "\n",
    "**Padding** is necessary if the convolutional filter is centered in the edge pixels of the image (e.g., fill imaginary pixels with value 0); if we match the top-left corner of the filter with the image, no padding is used.\n",
    "\n",
    "We move the filter left-right & top-down with a **stride** of `s` pixels; it somehow defines how fast we move with the kernel/filter on the image.\n",
    "\n",
    "**Filters** are `n x n` **kernels** that are applied to transfom pixel values according to the neighboring pixel-value distribution. Filter weights are the ones that are estimated during training. The result of applying a filter to an image is a **feature map**, aka. **activation map**.\n",
    "\n",
    "In addition to convolutions, we can use **pooling**: in a local neighborhood (eg., `2 x 2` window) of the image/feature map, we take a summarizing pixel value, eg.: maximum, average, etc.\n",
    "\n",
    "It is common to use **dropout**, too: neurons are randomly de-activated in a feedforward pass; this has the effect of preventing **overfitting**.\n",
    "\n",
    "The image size is **compressed** as we apply convolutions and pooling. Deeper layers account for higher level features.\n",
    "\n",
    "Note: convolutions are not exclusive of images; we can apply them to **1D** data, too (e.g., a series); in the special case of images we have **2D** convolutions with 2D filters.\n",
    "\n",
    "Popular neural network architectures (they are basically a set of designed layers):\n",
    "- LeNet-5\n",
    "- AlexNet\n",
    "- GoogLeNet\n",
    "- ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST dataset is a popular benchmarking dataset in which we ave samples of hand-written symbols for 0-9 digits. Here are the properties:\n",
    "- 60k training images, 10k test images\n",
    "- Each image is 28x28 pixels, single channel\n",
    "- Pixel values are normalized to [0,1]\n",
    "- The dataset is a 4-dimensional array:\n",
    "    (Samples, Width, Height, Channels) -> (60000, 28, 28, 1)\n",
    "- Labels come in an array in which each element is the digit label 0-9; but we transform it into the **one-hot-encondig**: 0/1 for each of the 0-9 classes, i.e.: (Samples, Classes) -> (60000, 10); 4 = (0,0,0,0,1,0,0,0,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train), (x_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdaf67a7470>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAADgpJREFUeJzt3X+MVfWZx/HPs1j+kKI4aQRCYSnEYJW4082IjSWrxkzVDQZHrekkJjQapn8wiU02ZA3/VNNgyCrslmiamaZYSFpKE3VB0iw0otLGZuKIWC0srTFsO3IDNTjywx9kmGf/mEMzxbnfe+fec++5zPN+JeT+eM6558kNnznn3O+592vuLgDx/EPRDQAoBuEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxDUZc3cmJlxOSHQYO5u1SxX157fzO40syNm9q6ZPVrPawFoLqv12n4zmybpj5I6JQ1Jel1St7sfSqzDnh9osGbs+ZdJetfd33P3c5J+IWllHa8HoInqCf88SX8Z93goe+7vmFmPmQ2a2WAd2wKQs3o+8Jvo0OJzh/Xu3i+pX+KwH2gl9ez5hyTNH/f4y5KO1dcOgGapJ/yvS7rGzL5iZtMlfVvSrnzaAtBoNR/2u/uImfVK2iNpmqQt7v6H3DoD0FA1D/XVtDHO+YGGa8pFPgAuXYQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EVfMU3ZJkZkclnZZ0XtKIu3fk0RTyM23atGT9yiuvbOj2e3t7y9Yuv/zy5LpLlixJ1tesWZOsP/XUU2Vr3d3dyXU//fTTZH3Dhg3J+uOPP56st4K6wp+5zd0/yOF1ADQRh/1AUPWG3yXtNbM3zKwnj4YANEe9h/3fcPdjZna1pF+b2f+6+/7xC2R/FPjDALSYuvb87n4suz0h6QVJyyZYpt/dO/gwEGgtNYffzGaY2cwL9yV9U9I7eTUGoLHqOeyfLekFM7vwOj939//JpSsADVdz+N39PUn/lGMvU9aCBQuS9enTpyfrN998c7K+fPnysrVZs2Yl173vvvuS9SINDQ0l65s3b07Wu7q6ytZOnz6dXPett95K1l999dVk/VLAUB8QFOEHgiL8QFCEHwiK8ANBEX4gKHP35m3MrHkba6L29vZkfd++fcl6o79W26pGR0eT9YceeihZP3PmTM3bLpVKyfqHH36YrB85cqTmbTeau1s1y7HnB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgGOfPQVtbW7I+MDCQrC9atCjPdnJVqffh4eFk/bbbbitbO3fuXHLdqNc/1ItxfgBJhB8IivADQRF+ICjCDwRF+IGgCD8QVB6z9IZ38uTJZH3t2rXJ+ooVK5L1N998M1mv9BPWKQcPHkzWOzs7k/WzZ88m69dff33Z2iOPPJJcF43Fnh8IivADQRF+ICjCDwRF+IGgCD8QFOEHgqr4fX4z2yJphaQT7r40e65N0g5JCyUdlfSAu6d/6FxT9/v89briiiuS9UrTSff19ZWtPfzww8l1H3zwwWR9+/btyTpaT57f5/+ppDsveu5RSS+5+zWSXsoeA7iEVAy/u++XdPElbCslbc3ub5V0T859AWiwWs/5Z7t7SZKy26vzawlAMzT82n4z65HU0+jtAJicWvf8x81sriRltyfKLeju/e7e4e4dNW4LQAPUGv5dklZl91dJ2plPOwCapWL4zWy7pN9JWmJmQ2b2sKQNkjrN7E+SOrPHAC4hFc/53b27TOn2nHsJ69SpU3Wt/9FHH9W87urVq5P1HTt2JOujo6M1bxvF4go/ICjCDwRF+IGgCD8QFOEHgiL8QFBM0T0FzJgxo2ztxRdfTK57yy23JOt33XVXsr53795kHc3HFN0Akgg/EBThB4Ii/EBQhB8IivADQRF+ICjG+ae4xYsXJ+sHDhxI1oeHh5P1l19+OVkfHBwsW3vmmWeS6zbz/+ZUwjg/gCTCDwRF+IGgCD8QFOEHgiL8QFCEHwiKcf7gurq6kvVnn302WZ85c2bN2163bl2yvm3btmS9VCrVvO2pjHF+AEmEHwiK8ANBEX4gKMIPBEX4gaAIPxBUxXF+M9siaYWkE+6+NHvuMUmrJf01W2ydu/+q4sYY57/kLF26NFnftGlTsn777bXP5N7X15esr1+/Pll///33a972pSzPcf6fSrpzguf/093bs38Vgw+gtVQMv7vvl3SyCb0AaKJ6zvl7zez3ZrbFzK7KrSMATVFr+H8kabGkdkklSRvLLWhmPWY2aGblf8wNQNPVFH53P+7u5919VNKPJS1LLNvv7h3u3lFrkwDyV1P4zWzuuIddkt7Jpx0AzXJZpQXMbLukWyV9ycyGJH1f0q1m1i7JJR2V9N0G9gigAfg+P+oya9asZP3uu+8uW6v0WwFm6eHqffv2JeudnZ3J+lTF9/kBJBF+ICjCDwRF+IGgCD8QFOEHgmKoD4X57LPPkvXLLktfhjIyMpKs33HHHWVrr7zySnLdSxlDfQCSCD8QFOEHgiL8QFCEHwiK8ANBEX4gqIrf50dsN9xwQ7J+//33J+s33nhj2VqlcfxKDh06lKzv37+/rtef6tjzA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQjPNPcUuWLEnWe3t7k/V77703WZ8zZ86ke6rW+fPnk/VSqZSsj46O5tnOlMOeHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCqjjOb2bzJW2TNEfSqKR+d/+hmbVJ2iFpoaSjkh5w9w8b12pclcbSu7u7y9YqjeMvXLiwlpZyMTg4mKyvX78+Wd+1a1ee7YRTzZ5/RNK/uftXJX1d0hozu07So5JecvdrJL2UPQZwiagYfncvufuB7P5pSYclzZO0UtLWbLGtku5pVJMA8jepc34zWyjpa5IGJM1295I09gdC0tV5Nwegcaq+tt/MvijpOUnfc/dTZlVNByYz65HUU1t7ABqlqj2/mX1BY8H/mbs/nz193MzmZvW5kk5MtK6797t7h7t35NEwgHxUDL+N7eJ/Iumwu28aV9olaVV2f5Wknfm3B6BRKk7RbWbLJf1G0tsaG+qTpHUaO+//paQFkv4s6VvufrLCa4Wconv27NnJ+nXXXZesP/3008n6tddeO+me8jIwMJCsP/nkk2VrO3em9xd8Jbc21U7RXfGc391/K6nci90+maYAtA6u8AOCIvxAUIQfCIrwA0ERfiAowg8ExU93V6mtra1sra+vL7lue3t7sr5o0aKaesrDa6+9lqxv3LgxWd+zZ0+y/sknn0y6JzQHe34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCCrMOP9NN92UrK9duzZZX7ZsWdnavHnzauopLx9//HHZ2ubNm5PrPvHEE8n62bNna+oJrY89PxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EFWacv6urq656PQ4dOpSs7969O1kfGRlJ1lPfuR8eHk6ui7jY8wNBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUObu6QXM5kvaJmmOpFFJ/e7+QzN7TNJqSX/NFl3n7r+q8FrpjQGom7tbNctVE/65kua6+wEzmynpDUn3SHpA0hl3f6rapgg/0HjVhr/iFX7uXpJUyu6fNrPDkor96RoAdZvUOb+ZLZT0NUkD2VO9ZvZ7M9tiZleVWafHzAbNbLCuTgHkquJh/98WNPuipFclrXf3581stqQPJLmkH2js1OChCq/BYT/QYLmd80uSmX1B0m5Je9x90wT1hZJ2u/vSCq9D+IEGqzb8FQ/7zcwk/UTS4fHBzz4IvKBL0juTbRJAcar5tH+5pN9IeltjQ32StE5St6R2jR32H5X03ezDwdRrsecHGizXw/68EH6g8XI77AcwNRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCavYU3R9I+r9xj7+UPdeKWrW3Vu1Lorda5dnbP1a7YFO/z/+5jZsNuntHYQ0ktGpvrdqXRG+1Kqo3DvuBoAg/EFTR4e8vePsprdpbq/Yl0VutCumt0HN+AMUpes8PoCCFhN/M7jSzI2b2rpk9WkQP5ZjZUTN728wOFj3FWDYN2gkze2fcc21m9msz+1N2O+E0aQX19piZvZ+9dwfN7F8L6m2+mb1sZofN7A9m9kj2fKHvXaKvQt63ph/2m9k0SX+U1ClpSNLrkrrd/VBTGynDzI5K6nD3wseEzexfJJ2RtO3CbEhm9h+STrr7huwP51Xu/u8t0ttjmuTMzQ3qrdzM0t9Rge9dnjNe56GIPf8ySe+6+3vufk7SLyStLKCPlufu+yWdvOjplZK2Zve3auw/T9OV6a0luHvJ3Q9k909LujCzdKHvXaKvQhQR/nmS/jLu8ZBaa8pvl7TXzN4ws56im5nA7AszI2W3Vxfcz8UqztzcTBfNLN0y710tM17nrYjwTzSbSCsNOXzD3f9Z0l2S1mSHt6jOjyQt1tg0biVJG4tsJptZ+jlJ33P3U0X2Mt4EfRXyvhUR/iFJ88c9/rKkYwX0MSF3P5bdnpD0gsZOU1rJ8QuTpGa3Jwru52/c/bi7n3f3UUk/VoHvXTaz9HOSfubuz2dPF/7eTdRXUe9bEeF/XdI1ZvYVM5su6duSdhXQx+eY2YzsgxiZ2QxJ31TrzT68S9Kq7P4qSTsL7OXvtMrMzeVmllbB712rzXhdyEU+2VDGf0maJmmLu69vehMTMLNFGtvbS2PfePx5kb2Z2XZJt2rsW1/HJX1f0n9L+qWkBZL+LOlb7t70D97K9HarJjlzc4N6Kzez9IAKfO/ynPE6l364wg+IiSv8gKAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8E9f/Ex0YKZYOZcwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img, cmap='gray') # to reverse cmap, append _r: 'gray_r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We transform y to one-hot\n",
    "# in order to avoid the network to think it's some kind of regression\n",
    "# We could do it with a for loop\n",
    "# but there's a utility for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat_test = to_categorical(y_test,10)\n",
    "y_cat_train = to_categorical(y_train,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to normalize the pixel values: [0,255] -> [0,1]\n",
    "img.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test / x_test.max()\n",
    "x_train = x_train / x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to reshape x to have a field for channels\n",
    "# eventhough we have a single channel in our example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((60000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "# We load all layer types we're going to use\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the model with the layers\n",
    "model = Sequential()\n",
    "\n",
    "# CONVOLUTIONAL layer:\n",
    "# 32 filters is quite standard (if complex images, more)\n",
    "# 4x4 size for the filter kernels is also standard (3-4)\n",
    "# input shape (for each image) is given by the dataset image shape\n",
    "model.add(Conv2D(filters=32, kernel_size=(4,4), input_shape=(28,28,1), activation='relu'))\n",
    "\n",
    "# POOLING layer:\n",
    "# 2x2 is also quiste a standard size for max-pooling\n",
    "# Note: in the new version of keras it's called MaxPooling2D - but MaxPool2D also works\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# FLATTEN layer: 2D (images) --> 1D (for later class array)\n",
    "model.add(Flatten())\n",
    "\n",
    "# Map the flattened array to the class array with a fully connected layer\n",
    "# The mapping is done in 2 steps (that's also quite usual)\n",
    "# 28x28 = 784\n",
    "# 28 - (3) = 25 -> 25x25 = 625; 4x4 kernel without padding and stride=1 results in 25 pixels\n",
    "# floor(25/2)*floor(25/2) = 12x12 = 144; max-pool: every 4 pixels 1 (max) taken\n",
    "# 144 -> 128 -> 10 (classes)\n",
    "# Often powers of 2 are used as steps: 64, 128, 256, 512, 1024, ...\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax')) # last activation to classes requires softmax for probabilities\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 25, 25, 32)        544       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 591,786\n",
      "Trainable params: 591,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 526s 9ms/step - loss: 0.1380 - acc: 0.9582\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 507s 8ms/step - loss: 0.0504 - acc: 0.9851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fedd4223630>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model.fit(x_train, y_cat_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation\n",
    "# Get metrics used to evaluate: loss, acc\n",
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 25s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.040847731342737094, 0.9868]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test set should be similar to train set in terms of accuracy\n",
    "# If train set is much better, we have overfitted\n",
    "# loss, acc\n",
    "model.evaluate(x_test, y_cat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT NOTE:\n",
    "# model.predict_classes() does not deliver predictions as one-hot\n",
    "# but as class categories 0-9\n",
    "# HOWEVER, note that we train with categorical y values!\n",
    "# This difference is relevant for the classification report\n",
    "y_pred = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99       980\n",
      "          1       0.98      1.00      0.99      1135\n",
      "          2       0.98      0.98      0.98      1032\n",
      "          3       0.99      0.99      0.99      1010\n",
      "          4       0.99      0.99      0.99       982\n",
      "          5       0.99      0.99      0.99       892\n",
      "          6       0.99      0.98      0.98       958\n",
      "          7       0.99      0.98      0.98      1028\n",
      "          8       0.99      0.98      0.98       974\n",
      "          9       0.99      0.98      0.98      1009\n",
      "\n",
      "avg / total       0.99      0.99      0.99     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We get very good results\n",
    "# This type of standard conv networks (withthe architecture we defined)\n",
    "# seems to work very well for hand-written digits\n",
    "print(classification_report(y_test, y_pred)) # labels, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 53s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train,y_train), (x_test,y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 32x32 images, 3 channels\n",
    "# train: 50k images\n",
    "# test: 10k images\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fed804a9c18>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "x_test = x_test / x_test.max()\n",
    "x_train = x_train / x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6],\n",
       "       [9],\n",
       "       [9],\n",
       "       ...,\n",
       "       [9],\n",
       "       [1],\n",
       "       [1]], dtype=uint8)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to one-hot encoding\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat_test = to_categorical(y_test,10)\n",
    "y_cat_train = to_categorical(y_train,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the category names from Wikipedia:\n",
    "# https://en.wikipedia.org/wiki/CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['airplane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(category_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the model with the layers\n",
    "model = Sequential()\n",
    "\n",
    "# CONVOLUTIONAL layer 1\n",
    "model.add(Conv2D(filters=32, kernel_size=(4,4), input_shape=(32,32,3), activation='relu'))\n",
    "# POOLING layer:\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# CONVOLUTIONAL layer 2: since images are more complex, we add another conv-maxpool pair\n",
    "model.add(Conv2D(filters=32, kernel_size=(4,4), activation='relu'))\n",
    "# POOLING layer:\n",
    "# Note: in the new version of keras it's called MaxPooling2D - but MaxPool2D also works\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# FLATTEN layer: 2D (images) --> 1D (for later class array)\n",
    "model.add(Flatten())\n",
    "\n",
    "# Map the flattened array to the class array with a fully connected layer\n",
    "# Often powers of 2 are used; 512 gives a slightly better performance\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax')) # last activation to classes requires softmax for probabilities\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 29, 29, 32)        1568      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 11, 11, 32)        16416     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               205056    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 225,610\n",
      "Trainable params: 225,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate\n",
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 102s 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3785230078697204, 0.5152]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_cat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.72      0.55      1000\n",
      "          1       0.68      0.58      0.63      1000\n",
      "          2       0.61      0.14      0.23      1000\n",
      "          3       0.47      0.21      0.29      1000\n",
      "          4       0.51      0.41      0.45      1000\n",
      "          5       0.53      0.41      0.46      1000\n",
      "          6       0.68      0.54      0.60      1000\n",
      "          7       0.48      0.71      0.57      1000\n",
      "          8       0.55      0.69      0.61      1000\n",
      "          9       0.42      0.73      0.54      1000\n",
      "\n",
      "avg / total       0.54      0.52      0.49     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred)) # labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
