{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Deep Learning with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Assignment: Training and Inference with FMNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset and prepare it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 1us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 2s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train,y_train), (x_test,y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdf38adbb70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 0, 3, 0], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/zalandoresearch/fashion-mnist\n",
    "class_names = ['tshirt_top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle_boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to one-hot format/encoding\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat_test = to_categorical(y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat_train = to_categorical(y_train,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale\n",
    "x_test = x_test / x_test.max()\n",
    "x_train = x_train / x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((60000, 28, 28, 1))\n",
    "x_test = x_test.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "# We load all layer types we're going to use\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the model with the layers\n",
    "model = Sequential()\n",
    "\n",
    "# CONVOLUTIONAL layer:\n",
    "# 32 filters is quite standard (if complex images, more)\n",
    "# 4x4 size for the filter kernels is also standard (3-4)\n",
    "# input shape (for each image) is given by the dataset image shape\n",
    "model.add(Conv2D(filters=32, kernel_size=(4,4), input_shape=(28,28,1), activation='relu'))\n",
    "\n",
    "# POOLING layer:\n",
    "# 2x2 is also quiste a standard size for max-pooling\n",
    "# Note: in the new version of keras it's called MaxPooling2D - but MaxPool2D also works\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# FLATTEN layer: 2D (images) --> 1D (for later class array)\n",
    "model.add(Flatten())\n",
    "\n",
    "# Map the flattened array to the class array with a fully connected layer\n",
    "# The mapping is done in 2 steps (that's also quite usual)\n",
    "# 28x28 = 784\n",
    "# 28 - (3) = 25 -> 25x25 = 625; 4x4 kernel without padding and stride=1 results in 25 pixels\n",
    "# floor(25/2)*floor(25/2) = 12x12 = 144; max-pool: every 4 pixels 1 (max) taken\n",
    "# 144 -> 128 -> 10 (classes)\n",
    "# Often powers of 2 are used as steps: 64, 128, 256, 512, 1024, ...\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax')) # last activation to classes requires softmax for probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 25, 25, 32)        544       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 591,786\n",
      "Trainable params: 591,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 518s 9ms/step - loss: 0.3950 - acc: 0.8594\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 514s 9ms/step - loss: 0.2757 - acc: 0.9015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdf3eb9f748>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model.fit(x_train, y_cat_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation\n",
    "# Get metrics used to evaluate: loss, acc\n",
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 33s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27677204971313474, 0.8997]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test set should be similar to train set in terms of accuracy\n",
    "# If train set is much better, we have overfitted\n",
    "# loss, acc\n",
    "model.evaluate(x_test, y_cat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT NOTE:\n",
    "# model.predict_classes() does not deliver predictions as one-hot\n",
    "# but as class categories 0-9\n",
    "# HOWEVER, note that we train with categorical y values!\n",
    "# This difference is relevant for the classification report\n",
    "y_pred = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1, ..., 8, 1, 5])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.84      0.85      1000\n",
      "          1       0.99      0.98      0.98      1000\n",
      "          2       0.83      0.82      0.83      1000\n",
      "          3       0.87      0.94      0.90      1000\n",
      "          4       0.83      0.84      0.84      1000\n",
      "          5       0.97      0.98      0.98      1000\n",
      "          6       0.74      0.69      0.71      1000\n",
      "          7       0.96      0.95      0.96      1000\n",
      "          8       0.97      0.98      0.98      1000\n",
      "          9       0.96      0.97      0.97      1000\n",
      "\n",
      "avg / total       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We get very good results\n",
    "# This type of standard conv networks (withthe architecture we defined)\n",
    "# seems to work very well for hand-written digits\n",
    "print(classification_report(y_test, y_pred)) # labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
